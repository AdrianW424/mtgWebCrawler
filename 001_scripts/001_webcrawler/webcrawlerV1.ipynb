{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = r'https://gatherer.wizards.com/Pages/Search/Default.aspx?page=0&name=+[]'\n",
    "\n",
    "# gibt html code der gewünschten url zurück\n",
    "def get_url_content(url):\n",
    "    return requests.get(url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "from urllib3 import disable_warnings\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "\n",
    "disable_warnings(InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(url, verify=False)\n",
    "\n",
    "print(page.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = ''\n",
    "i = 0\n",
    "while(i < 2):\n",
    "    data = requests.get(url, verify=False)\n",
    "    if data != prev:\n",
    "        prev = data\n",
    "        i += 1\n",
    "    else:\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "i = 0\n",
    "while(i < 1):\n",
    "    data.append(requests.get(\"https://gatherer.wizards.com/Pages/Search/Default.aspx?page=\"+str(i)+\"&name=+[]\", verify=False))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def hallo():\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from time import perf_counter\n",
    "import aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "25\n",
      "time taken: 26.02024579999852\n"
     ]
    }
   ],
   "source": [
    "def __checkForEndOfPages(html, page):\n",
    "    \"\"\"checks if the underlined pagenumber in the @html string is equivilant to the number given as @page\n",
    "\n",
    "    Args:\n",
    "        html (str): html-string containing one page\n",
    "        page (int): page number\n",
    "\n",
    "    Returns:\n",
    "        bool: if the page numbers are the same -> False (not end of Pages). If page numbers are not the same -> True\n",
    "    \"\"\"\n",
    "    return int(BeautifulSoup(html, 'html.parser').find('div', class_='simpleRoundedBoxTitleGreyTall').find('div', class_='pagingcontrols').find('a', style='text-decoration:underline;').contents[0]) != page\n",
    "\n",
    "def __deleteRedundantPages(res, div):\n",
    "    \"\"\"delete all pages from the end, that are redundant\n",
    "\n",
    "    Args:\n",
    "        res (list of string): list that contains all pages\n",
    "        div (int): amount of pages per iteration\n",
    "\n",
    "    Returns:\n",
    "        list of string: list that contains all pages (now without redundant pages at the end)\n",
    "    \"\"\"\n",
    "    if div == 1:\n",
    "        res = res[:-1]\n",
    "    else:\n",
    "        buf = res[-div:]\n",
    "        res = res[:-div]\n",
    "        \n",
    "        parse_buffer1 = int(BeautifulSoup(buf[-1], 'html.parser').find('div', class_='simpleRoundedBoxTitleGreyTall').find('div', class_='pagingcontrols').find('a', style='text-decoration:underline;').contents[0])\n",
    "        for i in reversed(range(1, div)):\n",
    "            parse_buffer2 = int(BeautifulSoup(buf[i-1], 'html.parser').find('div', class_='simpleRoundedBoxTitleGreyTall').find('div', class_='pagingcontrols').find('a', style='text-decoration:underline;').contents[0])\n",
    "            if parse_buffer1 == parse_buffer2:\n",
    "                buf.pop()\n",
    "                if i == 1 and len(res) > 0:\n",
    "                    if int(BeautifulSoup(buf[i-1], 'html.parser').find('div', class_='simpleRoundedBoxTitleGreyTall').find('div', class_='pagingcontrols').find('a', style='text-decoration:underline;').contents[0]) == int(BeautifulSoup(res[-1], 'html.parser').find('div', class_='simpleRoundedBoxTitleGreyTall').find('div', class_='pagingcontrols').find('a', style='text-decoration:underline;').contents[0]):\n",
    "                        buf.pop()\n",
    "            else:\n",
    "                break\n",
    "            parse_buffer1 = parse_buffer2\n",
    "        res += buf\n",
    "    return res\n",
    "\n",
    "async def fetch(s, url):\n",
    "    async with s.get(f'https://gatherer.wizards.com/Pages/Search/Default.aspx?sort=cn+&page={url}&name=%20[]') as r:\n",
    "        if r.status != 200:\n",
    "            r.raise_for_status()\n",
    "        return await r.text()\n",
    "\n",
    "async def fetch_all(s, pageNum, offset, div):\n",
    "    tasks = []\n",
    "    res = []\n",
    "    end = math.ceil(pageNum/div)\n",
    "    if pageNum != 0:\n",
    "        for i in range(int(offset/div), end + int(offset/div)):\n",
    "            for i2 in range(i*div, (i + 1)*div):\n",
    "                task = asyncio.create_task(fetch(s, i2))\n",
    "                tasks.append(task)\n",
    "            res = await asyncio.gather(*tasks)\n",
    "            print(i)\n",
    "            if __checkForEndOfPages(res[-1], (i + 1)*div):\n",
    "                res = __deleteRedundantPages(res, div)\n",
    "                break  \n",
    "        if pageNum < len(res):\n",
    "            res = res[:pageNum]\n",
    "    else:\n",
    "        i = int(offset/div)\n",
    "        while 1:\n",
    "            for i2 in range(i*div, (i + 1)*div):\n",
    "                task = asyncio.create_task(fetch(s, i2))\n",
    "                tasks.append(task)\n",
    "            res = await asyncio.gather(*tasks)\n",
    "            print(i)\n",
    "            if __checkForEndOfPages(res[-1], (i + 1)*div):\n",
    "                res = __deleteRedundantPages(res, div)\n",
    "                break\n",
    "            i += 1\n",
    "    return res\n",
    "\n",
    "async def main(pageNum = 0):\n",
    "    div = 10        # integer\n",
    "    offset = 249    # integer - sollte Vielfaches von div sein - bei div = 10 -> offset = 10 oder = 20 oder ... oder = 250\n",
    "                    # wird als offset = 241 gewählt, so wird intern daraus 240 gemacht, genauso bei offset = 242 oder ... oder 249 ... usw.\n",
    "    if div < 1:\n",
    "        print(\"Div muss >= 1\")\n",
    "    else:\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            htmls = await fetch_all(session, pageNum, offset, div)\n",
    "            #print(htmls)\n",
    "            return htmls\n",
    "\n",
    "start = perf_counter()\n",
    "#asyncio.run(main())\n",
    "htmls = await main(0)\n",
    "stop = perf_counter()\n",
    "print(\"time taken:\", stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__checkForEndOfPages(htmls[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(htmls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(htmls[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "results = BeautifulSoup(htmls[0], 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2344353\n"
     ]
    }
   ],
   "source": [
    "print(re.search(\"multiverseid=(.*)&\", url).group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImageAndID(card):\n",
    "    data = card.find('td', class_='leftCol').find('a').find('img')['src'][5:]\n",
    "    return \"https://gatherer.wizards.com\" + data, re.search(\"multiverseid=(.*)&\", data).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getName(card):\n",
    "    return card.find('td', class_='middleCol').find('div', class_='cardInfo').find('span', class_='cardTitle').find('a').contents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getType(card):\n",
    "    return re.sub(' +', ' ', re.sub(r'\\r\\n', \"\", str(card.find('td', class_='middleCol').find('div', class_='cardInfo').find('span', class_='typeLine').contents[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards = results.find_all('tr', class_=['cardItem evenItem', 'cardItem oddItem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Creature — Human Cleric (1/3)'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(' +', ' ', re.sub(r'\\r\\n', \"\", getType(cards[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for html in htmls:\n",
    "    results = BeautifulSoup(html, 'html.parser')\n",
    "    cards = results.find_all('tr', class_=['cardItem evenItem', 'cardItem oddItem'])\n",
    "    #print(i)\n",
    "    for card in cards:\n",
    "        print(getName(card))\n",
    "        print(getImageAndID(card))\n",
    "        print(getType(card))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class csvCreator():\n",
    "    \n",
    "    def __init__(this):\n",
    "        this.__csvCols = []\n",
    "        this.__csvConstruct = []\n",
    "        this.__csv = ''\n",
    "    \n",
    "    def defineCol(this, *params):\n",
    "        this.__csvCols = []\n",
    "        for param in params:\n",
    "            this.__csvCols.append(param)\n",
    "    \n",
    "    def addCard(this, *params):\n",
    "        this.__csvConstruct.append([])\n",
    "        for param in params:\n",
    "            this.__csvConstruct[-1].append(param)\n",
    "    \n",
    "    def getCsv(this):\n",
    "        this.__csv = this.__printRow(this.__csvCols)\n",
    "        for row in this.__csvConstruct:\n",
    "            this.__csv += this.__printRow(row)\n",
    "        return this.__csv\n",
    "    \n",
    "    def __printRow(this, row):\n",
    "        buf = \"\"\n",
    "        for valorem in row:\n",
    "            buf += str(valorem) + \",\"\n",
    "        return buf[:-1] + \"\\n\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = csvCreator()\n",
    "csv.defineCol(\"id\", \"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.addCard(1, \"Adrian\")\n",
    "csv.addCard(2, \"Adrian2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,name\n",
      "1,Adrian\n",
      "2,Adrian2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(csv.getCsv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse html-input as json\n",
    "\n",
    "for html in htmls:\n",
    "    results = BeautifulSoup(html, 'html.parser')\n",
    "    cards = results.find_all('tr', class_=['cardItem evenItem', 'cardItem oddItem'])\n",
    "    #print(i)\n",
    "    for card in cards:\n",
    "        print(getName(card))\n",
    "        print(getImageAndID(card))\n",
    "        print(getType(card))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined crawler-data with csvCreator\n",
    "\n",
    "import re\n",
    "\n",
    "csv = csvCreator()\n",
    "csv.defineCol(\"ID\", \"Image-ID\", \"Name\", \"Type\")\n",
    "\n",
    "for html in htmls:\n",
    "    results = BeautifulSoup(html, 'html.parser')\n",
    "    cards = results.find_all('tr', class_=['cardItem evenItem', 'cardItem oddItem'])\n",
    "    #print(i)\n",
    "    for card in cards:\n",
    "        img, id = getImageAndID(card)\n",
    "        csv.addCard(id, img, getName(card), getType(card))\n",
    "    \n",
    "f = open(\"Lukas.csv\", \"w\")\n",
    "f.write(csv.getCsv())\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc33baed695a5bb14335e30a3d3eed70e7505dd1711acefe1e2909fc8eedfc84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
